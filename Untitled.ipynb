{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5061dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories and parameters\n",
    "train_dir = 'images/train'\n",
    "test_dir = 'images/validation'\n",
    "img_height = 80\n",
    "img_width = 80\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c1c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load the data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e660d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(train_data.num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba24a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a95119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "900/900 [==============================] - 82s 90ms/step - loss: 1.7509 - accuracy: 0.2878 - val_loss: 1.5666 - val_accuracy: 0.3912\n",
      "Epoch 2/50\n",
      "900/900 [==============================] - 80s 89ms/step - loss: 1.6016 - accuracy: 0.3739 - val_loss: 1.4574 - val_accuracy: 0.4467\n",
      "Epoch 3/50\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 1.5012 - accuracy: 0.4193 - val_loss: 1.3583 - val_accuracy: 0.4768\n",
      "Epoch 4/50\n",
      "900/900 [==============================] - 78s 87ms/step - loss: 1.4456 - accuracy: 0.4440 - val_loss: 1.3018 - val_accuracy: 0.5014\n",
      "Epoch 5/50\n",
      "900/900 [==============================] - 78s 87ms/step - loss: 1.3967 - accuracy: 0.4613 - val_loss: 1.2724 - val_accuracy: 0.5135\n",
      "Epoch 6/50\n",
      "900/900 [==============================] - 80s 89ms/step - loss: 1.3692 - accuracy: 0.4756 - val_loss: 1.2517 - val_accuracy: 0.5295\n",
      "Epoch 7/50\n",
      "900/900 [==============================] - 86s 95ms/step - loss: 1.3408 - accuracy: 0.4872 - val_loss: 1.2455 - val_accuracy: 0.5311\n",
      "Epoch 8/50\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 1.3279 - accuracy: 0.4920 - val_loss: 1.2325 - val_accuracy: 0.5318\n",
      "Epoch 9/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.3086 - accuracy: 0.5015 - val_loss: 1.1870 - val_accuracy: 0.5500\n",
      "Epoch 10/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.2861 - accuracy: 0.5111 - val_loss: 1.1905 - val_accuracy: 0.5463\n",
      "Epoch 11/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.2769 - accuracy: 0.5123 - val_loss: 1.1800 - val_accuracy: 0.5524\n",
      "Epoch 12/50\n",
      "900/900 [==============================] - 83s 93ms/step - loss: 1.2655 - accuracy: 0.5175 - val_loss: 1.1491 - val_accuracy: 0.5584\n",
      "Epoch 13/50\n",
      "900/900 [==============================] - 84s 93ms/step - loss: 1.2592 - accuracy: 0.5213 - val_loss: 1.1670 - val_accuracy: 0.5602\n",
      "Epoch 14/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.2522 - accuracy: 0.5245 - val_loss: 1.1421 - val_accuracy: 0.5673\n",
      "Epoch 15/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.2395 - accuracy: 0.5288 - val_loss: 1.1489 - val_accuracy: 0.5594\n",
      "Epoch 16/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.2313 - accuracy: 0.5307 - val_loss: 1.1471 - val_accuracy: 0.5673\n",
      "Epoch 17/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.2227 - accuracy: 0.5320 - val_loss: 1.1481 - val_accuracy: 0.5653\n",
      "Epoch 18/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.2280 - accuracy: 0.5313 - val_loss: 1.1310 - val_accuracy: 0.5697\n",
      "Epoch 19/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.2202 - accuracy: 0.5346 - val_loss: 1.1416 - val_accuracy: 0.5673\n",
      "Epoch 20/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.2136 - accuracy: 0.5376 - val_loss: 1.1237 - val_accuracy: 0.5709\n",
      "Epoch 21/50\n",
      "900/900 [==============================] - 80s 89ms/step - loss: 1.2118 - accuracy: 0.5385 - val_loss: 1.1161 - val_accuracy: 0.5746\n",
      "Epoch 22/50\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 1.2068 - accuracy: 0.5400 - val_loss: 1.1267 - val_accuracy: 0.5778\n",
      "Epoch 23/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1956 - accuracy: 0.5438 - val_loss: 1.1284 - val_accuracy: 0.5719\n",
      "Epoch 24/50\n",
      "900/900 [==============================] - 81s 91ms/step - loss: 1.1949 - accuracy: 0.5425 - val_loss: 1.1380 - val_accuracy: 0.5713\n",
      "Epoch 25/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.1957 - accuracy: 0.5474 - val_loss: 1.1279 - val_accuracy: 0.5722\n",
      "Epoch 26/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1842 - accuracy: 0.5484 - val_loss: 1.1118 - val_accuracy: 0.5795\n",
      "Epoch 27/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.1792 - accuracy: 0.5507 - val_loss: 1.1191 - val_accuracy: 0.5828\n",
      "Epoch 28/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.1829 - accuracy: 0.5508 - val_loss: 1.1190 - val_accuracy: 0.5818\n",
      "Epoch 29/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.1729 - accuracy: 0.5534 - val_loss: 1.1090 - val_accuracy: 0.5776\n",
      "Epoch 30/50\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 1.1744 - accuracy: 0.5525 - val_loss: 1.1048 - val_accuracy: 0.5827\n",
      "Epoch 31/50\n",
      "900/900 [==============================] - 84s 94ms/step - loss: 1.1690 - accuracy: 0.5510 - val_loss: 1.1094 - val_accuracy: 0.5851\n",
      "Epoch 32/50\n",
      "900/900 [==============================] - 87s 97ms/step - loss: 1.1640 - accuracy: 0.5596 - val_loss: 1.1243 - val_accuracy: 0.5729\n",
      "Epoch 33/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1604 - accuracy: 0.5525 - val_loss: 1.1127 - val_accuracy: 0.5753\n",
      "Epoch 34/50\n",
      "900/900 [==============================] - 83s 92ms/step - loss: 1.1569 - accuracy: 0.5610 - val_loss: 1.1183 - val_accuracy: 0.5790\n",
      "Epoch 35/50\n",
      "900/900 [==============================] - 86s 96ms/step - loss: 1.1591 - accuracy: 0.5582 - val_loss: 1.0977 - val_accuracy: 0.5786\n",
      "Epoch 36/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1534 - accuracy: 0.5585 - val_loss: 1.1080 - val_accuracy: 0.5736\n",
      "Epoch 37/50\n",
      "900/900 [==============================] - 87s 97ms/step - loss: 1.1563 - accuracy: 0.5593 - val_loss: 1.0940 - val_accuracy: 0.5830\n",
      "Epoch 38/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1513 - accuracy: 0.5627 - val_loss: 1.0928 - val_accuracy: 0.5868\n",
      "Epoch 39/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1444 - accuracy: 0.5598 - val_loss: 1.0982 - val_accuracy: 0.5852\n",
      "Epoch 40/50\n",
      "900/900 [==============================] - 86s 95ms/step - loss: 1.1435 - accuracy: 0.5645 - val_loss: 1.1041 - val_accuracy: 0.5832\n",
      "Epoch 41/50\n",
      "900/900 [==============================] - 84s 93ms/step - loss: 1.1398 - accuracy: 0.5662 - val_loss: 1.1003 - val_accuracy: 0.5849\n",
      "Epoch 42/50\n",
      "900/900 [==============================] - 81s 89ms/step - loss: 1.1367 - accuracy: 0.5697 - val_loss: 1.1190 - val_accuracy: 0.5814\n",
      "Epoch 43/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1402 - accuracy: 0.5650 - val_loss: 1.0836 - val_accuracy: 0.5919\n",
      "Epoch 44/50\n",
      "900/900 [==============================] - 81s 89ms/step - loss: 1.1417 - accuracy: 0.5678 - val_loss: 1.0808 - val_accuracy: 0.5911\n",
      "Epoch 45/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.1371 - accuracy: 0.5682 - val_loss: 1.0943 - val_accuracy: 0.5844\n",
      "Epoch 46/50\n",
      "900/900 [==============================] - 81s 89ms/step - loss: 1.1290 - accuracy: 0.5703 - val_loss: 1.0954 - val_accuracy: 0.5787\n",
      "Epoch 47/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1283 - accuracy: 0.5715 - val_loss: 1.0780 - val_accuracy: 0.5862\n",
      "Epoch 48/50\n",
      "900/900 [==============================] - 87s 96ms/step - loss: 1.1280 - accuracy: 0.5707 - val_loss: 1.0903 - val_accuracy: 0.5933\n",
      "Epoch 49/50\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1263 - accuracy: 0.5708 - val_loss: 1.0870 - val_accuracy: 0.5871\n",
      "Epoch 50/50\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.1239 - accuracy: 0.5702 - val_loss: 1.0896 - val_accuracy: 0.5885\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_data.samples//train_data.batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=test_data.samples//test_data.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9451a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 4s 19ms/step - loss: 1.0893 - accuracy: 0.5889\n",
      "Test accuracy: 0.5888763070106506\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "model.save('microexpression2_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ec46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "The predicted microexpression is: happiness\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('microexpressions_model2.h5')\n",
    "\n",
    "\n",
    "# Load an image of a micro expression\n",
    "image = cv2.imread('microexpression1.jpg')\n",
    "\n",
    "# Preprocess the image in the same way as the training data\n",
    "image = cv2.resize(image, (80, 80))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make a prediction on the preprocessed image using the trained model\n",
    "prediction = model.predict(image)[0]\n",
    "\n",
    "# Get the label with the highest probability\n",
    "labels = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "label = labels[np.argmax(prediction)]\n",
    "\n",
    "# Print the predicted label\n",
    "print('The predicted microexpression is:', label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "205d1767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "The predicted microexpression is: anger\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('microexpressions_model2.h5')\n",
    "\n",
    "# Load an image of a micro expression\n",
    "image = cv2.imread('microexpression.jpg')\n",
    "\n",
    "# Preprocess the image in the same way as the training data\n",
    "image = cv2.resize(image, (80, 80))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make a prediction on the preprocessed image using the trained model\n",
    "prediction = model.predict(image)[0]\n",
    "\n",
    "# Get the label with the highest probability\n",
    "labels = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "label = labels[np.argmax(prediction)]\n",
    "\n",
    "# Print the predicted label\n",
    "print('The predicted microexpression is:', label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e526d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "The predicted microexpression is: happiness\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('microexpressions_model2.h5')\n",
    "\n",
    "# Load an image of a micro expression\n",
    "image = cv2.imread('anger.jpg')\n",
    "\n",
    "# Preprocess the image in the same way as the training data\n",
    "image = cv2.resize(image, (80, 80))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make a prediction on the preprocessed image using the trained model\n",
    "prediction = model.predict(image)[0]\n",
    "\n",
    "# Get the label with the highest probability\n",
    "labels = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "label = labels[np.argmax(prediction)]\n",
    "\n",
    "# Print the predicted label\n",
    "print('The predicted microexpression is:', label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0505610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "The predicted microexpression is: fear\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('microexpressions_model2.h5')\n",
    "\n",
    "# Load an image of a micro expression\n",
    "image = cv2.imread('533.jpg')\n",
    "\n",
    "# Preprocess the image in the same way as the training data\n",
    "image = cv2.resize(image, (80, 80))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make a prediction on the preprocessed image using the trained model\n",
    "prediction = model.predict(image)[0]\n",
    "\n",
    "# Get the label with the highest probability\n",
    "labels = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "label = labels[np.argmax(prediction)]\n",
    "\n",
    "# Print the predicted label\n",
    "print('The predicted microexpression is:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46d09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
